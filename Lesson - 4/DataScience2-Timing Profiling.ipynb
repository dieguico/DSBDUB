{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are sure that our program is correct, we can ask for performance. Given two different algorithms which solves the same problem with the same output, we can say that one gives better performance than the other by comparing its processing times.\n",
    "\n",
    "Time function gives us a time mark. It's straightforward to calculate a function processing time by taking marks before and after the function and subtract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sieveExample",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7df0419afd38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msieveExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msieveExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msieveOfEratosthenes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sieveExample"
     ]
    }
   ],
   "source": [
    "import sieveExample\n",
    "import time\n",
    "t1 = time.time()\n",
    "result = sieveExample.sieveOfEratosthenes(100000)\n",
    "t2 = time.time()\n",
    "print 'sieveOfEratosthenes took {} seconds'.format(t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take overall timing of a python script by passing `-t` argument to run command. \n",
    "This print timing information at the end of the run. IPython will give\n",
    "you an estimated CPU time consumption and wall clock times for your script. Under Unix, an estimate of time spent on system tasks is also given (for Windows platforms this is reported as 0.0, since it can not be measured). An additional ``-N<N>`` option can be given, where <N> must be an integer indicating how many times you want the script to\n",
    "run. The final timing report will include total and per run results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run?\n",
    "%run -t -N5 sieveExampleArgParse.py 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we want more timing with more precision we can use magic command %timeit. \n",
    "\n",
    "**%timeit** executes a function several times, and returns the best time obtained. \n",
    "It will limit the number of runs depending on how long the script takes to execute.\n",
    "\n",
    "The number of runs may be set with with -n 1000, for example, which will limit %timeit to a thousand iterations\n",
    "\n",
    "The number of rounds %timeit it is executed  can also be modified, using -r. For example -r will produce the best result of 5 executions, by default is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sieveExample\n",
    "%timeit sieveExample.sieveOfEratosthenes(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing two different implementations**\n",
    "\n",
    "Now lets to compare two different implimentations for the same problem. Numpy and scipy both have implemented a method for interpolation function. We can use %timeit to know which function is the fastest one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.interpolate as spip \n",
    "np.interp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spip.interp1d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2*np.pi, 10)\n",
    "y = np.sin(x)\n",
    "xvals = np.linspace(0, 2*np.pi, 50)\n",
    "#scipy\n",
    "f = spip.interp1d(x, y)\n",
    "scipy_vals = f(xvals)\n",
    "# numpy\n",
    "numpy_vals = np.interp(xvals, x, y)\n",
    "#assert if values are close!\n",
    "assert np.allclose(scipy_vals, numpy_vals) \n",
    "#np.allclose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have checked that both functions gives us the same result aproximately \n",
    "(absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`)); `rtol`=1e-05, `atol`=1e-08),\n",
    "\n",
    "we can check it performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'scipy:'\n",
    "%timeit -n 10000 -r5 f(xvals)\n",
    "print 'numpy:'\n",
    "%timeit -n 10000 -r5 np.interp(xvals, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the time consumed by an implementation, but this time can be shortened? And the key question how it can be done? First thing is to realize where is consuming more time in our code. Profiling gives us how many time takes each method or function been called. We can see the number of calls, total time, time per call and cummulative time since out function was called.\n",
    "\n",
    "At the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run -m cProfile sieveExampleArgs.py 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store profile results and visualize it with [`pstats`](http://docs.python.org/library/profile.html#module-pstats). From command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m cProfile -o sieveExample.prof sieveExampleArgs.py 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pstats\n",
    "stats = pstats.Stats('sieveExample.prof')\n",
    "stats.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.sort_stats('cumtime').print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.sort_stats('tottime').print_stats(5) #five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.sort_stats('cumtime').print_stats(r'range') #filter using Regular Expression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can use **magic command** %prun for profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%prun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sieveExample\n",
    "%prun sieveExample.sieveOfEratosthenes(100000) #Using magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%prun -D sieveExample_sieve.prof sieveExample.sieveOfEratosthenes(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%prun -q -D scipy_interp.prof f(xvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%prun -q -D numpy_interp.prof np.interp(xvals, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pstats\n",
    "stats = pstats.Stats('scipy_interp.prof')\n",
    "stats.sort_stats('tottime').print_stats(3) #three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pstats\n",
    "stats = pstats.Stats('numpy_interp.prof')\n",
    "stats.sort_stats('cumtime').print_stats(3) #three rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling can help to find useless calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "unames = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=unames, engine='python')\n",
    "rnames = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=rnames,  engine='python')\n",
    "mnames = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=mnames,  engine='python')\n",
    "data = pd.merge(pd.merge(ratings, users), movies)\n",
    "\n",
    "def top_movies(dataFrame,usr):\n",
    "    user= dataFrame[dataFrame.user_id == usr]\n",
    "    max_i = user.rating.max()\n",
    "    return user[user.rating == max_i].title\n",
    "\n",
    "def compareTopMovies(data,usr1, usr2):\n",
    "    movi1= top_movies(data,usr1).values\n",
    "    movi2 = top_movies(data,usr2).values\n",
    "    hits=np.intersect1d(movi1,movi2)\n",
    "    return hits\n",
    "\n",
    "#Top Movies for user 1\n",
    "print top_movies(data,1)\n",
    "#Compare TopMovies shared by two users:\n",
    "print compareTopMovies(data,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compare all users between them. Profiling\n",
    "%prun -D compare.prof {x:compareTopMovies(data,1,x) for x in users.user_id if x!=1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pstats\n",
    "stats = pstats.Stats('compare.prof')\n",
    "stats.sort_stats('cumtime').print_stats(50) #50 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can realize that TopMovie is called 2 times per each user in the table, inside compareTopMovies.\n",
    "Lets see these functions line per line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Line Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how long it took each line in a function to run.  Functions to profile this way must be passed by name with -f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##pip install line-profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun?\n",
    "%lprun -f top_movies top_movies(data,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -f compareTopMovies compareTopMovies(data,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Memory profiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look into memory profiling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##pip install psutil\n",
    "##pip install memory-profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much memory a script uses line by line. Let’s take a look at the sieveOfEratosthenes function that we profiled with %prun - except this time we’re interested in incremental memory usage and not execution time. NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mprun?\n",
    "#clear all variables\n",
    "#%reset \n",
    "import pandasExample\n",
    "%mprun -f pandasExample.test pandasExample.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much memory a script uses overall. %memit works a lot like %timeit except that the number of iterations is set with -r instead of -n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%memit -r 3  pandasExample.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Challenges\n",
    "\n",
    "1. Change the sieve Of Eratosthenes implemantion, such that its performances would be better. Hint: use Numpy arrays and boolean filters.\n",
    "\n",
    "2. Change function compareTopMovies in order to get better performance, by reducing useless code. Hint: reuse before recalculate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
